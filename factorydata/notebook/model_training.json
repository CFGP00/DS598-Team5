{
	"name": "model_training",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "model",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "aa7e009a-69b3-4ef4-b756-1ede0b91d34c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/3b608a14-90b7-42d9-82e2-77c947974137/resourceGroups/DS562-Team-5/providers/Microsoft.Synapse/workspaces/sentiment-analyses/bigDataPools/model",
				"name": "model",
				"type": "Spark",
				"endpoint": "https://sentiment-analyses.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/model",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from azure.storage.blob import BlobServiceClient\n",
					"from io import BytesIO\n",
					"import pandas as pd\n",
					"\n",
					"connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
					"\n",
					"container_name = \"team5container\"\n",
					"blob_path = \"Gold/Historical Stock/apple_ticker.parquet\"\n",
					"\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
					"blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
					"\n",
					"stream = BytesIO()\n",
					"blob_client.download_blob().readinto(stream)\n",
					"stream.seek(0)\n",
					"\n",
					"df_h = pd.read_parquet(stream)\n",
					"df_h.head()"
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"source": [
					"connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
					"\n",
					"container_name = \"team5container\"\n",
					"blob_path = \"Gold/News/news_with_sentiment.parquet\"\n",
					"\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
					"blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
					"\n",
					"stream = BytesIO()\n",
					"blob_client.download_blob().readinto(stream)\n",
					"stream.seek(0)\n",
					"\n",
					"df_n = pd.read_parquet(stream)\n",
					"df_n.head()"
				],
				"execution_count": 55
			},
			{
				"cell_type": "code",
				"source": [
					"connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
					"\n",
					"container_name = \"team5container\"\n",
					"blob_path = \"Gold/Product Reviews/reviews_with_sentiment.parquet\"\n",
					"\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
					"blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
					"\n",
					"stream = BytesIO()\n",
					"blob_client.download_blob().readinto(stream)\n",
					"stream.seek(0)\n",
					"\n",
					"df_r = pd.read_parquet(stream)\n",
					"df_r.head()"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"source": [
					"connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
					"\n",
					"container_name = \"team5container\"\n",
					"blob_path = \"Gold/Search Trends/trends_with_sentiment.parquet\"\n",
					"\n",
					"blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
					"blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_path)\n",
					"\n",
					"stream = BytesIO()\n",
					"blob_client.download_blob().readinto(stream)\n",
					"stream.seek(0)\n",
					"\n",
					"df_t = pd.read_parquet(stream)\n",
					"df_t.head()"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"source": [
					"# Strip column names to avoid hidden whitespaces\n",
					"for df in [df_n, df_r, df_t]:\n",
					"    df.columns = df.columns.str.strip().str.lower()\n",
					"\n",
					"# Convert 'sentiment' column from text to numeric\n",
					"sentiment_map = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
					"\n",
					"df_n['sentiment'] = df_n['sentiment'].map(sentiment_map)\n",
					"df_r['sentiment'] = df_r['sentiment'].map(sentiment_map)\n",
					"df_t['sentiment'] = df_t['sentiment'].map(sentiment_map)"
				],
				"execution_count": 58
			},
			{
				"cell_type": "code",
				"source": [
					"# Aggregate news sentiment\n",
					"df_news_daily = df_n.groupby('date', as_index=False)['sentiment'].mean()\n",
					"df_news_daily.rename(columns={'sentiment': 'sentiment_news'}, inplace=True)\n",
					"\n",
					"# Aggregate reviews sentiment\n",
					"df_reviews_daily = df_r.groupby('date', as_index=False)['sentiment'].mean()\n",
					"df_reviews_daily.rename(columns={'sentiment': 'sentiment_reviews'}, inplace=True)"
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"source": [
					"# Make sure date is datetime\n",
					"df_t['date'] = pd.to_datetime(df_t['date'])\n",
					"\n",
					"# Set date as index and resample to daily frequency\n",
					"df_search_daily = df_t.set_index('date').resample('D').ffill().reset_index()\n",
					"df_search_daily.rename(columns={'sentiment': 'sentiment_search'}, inplace=True)"
				],
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"source": [
					"# News sentiment\n",
					"df_news = df_n[['date', 'sentiment']].copy()\n",
					"df_news.rename(columns={'sentiment': 'sentiment_news'}, inplace=True)\n",
					"\n",
					"# Product reviews sentiment\n",
					"df_reviews = df_r[['date', 'sentiment']].copy()\n",
					"df_reviews.rename(columns={'sentiment': 'sentiment_reviews'}, inplace=True)\n",
					"\n",
					"# Search trend sentiment (already resampled in earlier step)\n",
					"df_search = df_search_daily[['date', 'sentiment_search']].copy()"
				],
				"execution_count": 61
			},
			{
				"cell_type": "code",
				"source": [
					"df_h['date'] = pd.to_datetime(df_h['date'])\n",
					"df_news['date'] = pd.to_datetime(df_news['date'])\n",
					"df_reviews['date'] = pd.to_datetime(df_reviews['date'])\n",
					"df_search['date'] = pd.to_datetime(df_search['date'])"
				],
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"source": [
					"df_h['date'] = pd.to_datetime(df_h['date'])  # ensure datetime\n",
					"\n",
					"df = df_h.merge(df_news, on='date', how='left') \\\n",
					"         .merge(df_reviews, on='date', how='left') \\\n",
					"         .merge(df_search, on='date', how='left')"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"source": [
					"df[['sentiment_news', 'sentiment_reviews', 'sentiment_search']] = df[\n",
					"    ['sentiment_news', 'sentiment_reviews', 'sentiment_search']\n",
					"].fillna(0)"
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"source": [
					"df.head(10)"
				],
				"execution_count": 65
			},
			{
				"cell_type": "code",
				"source": [
					"df.shape"
				],
				"execution_count": 72
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.preprocessing import MinMaxScaler\n",
					"\n",
					"# Select features and target\n",
					"features = ['Open', 'High', 'Low', 'Close', 'Volume', \n",
					"            'sentiment_news', 'sentiment_reviews', 'sentiment_search']\n",
					"\n",
					"# Create a copy to preserve original\n",
					"df_lstm = df[['date'] + features].copy()\n",
					"\n",
					"# Drop any remaining nulls (e.g., from missing sentiment data)\n",
					"df_lstm.dropna(inplace=True)\n",
					"\n",
					"# Initialize scaler\n",
					"scaler = MinMaxScaler()\n",
					"scaled_values = scaler.fit_transform(df_lstm[features])"
				],
				"execution_count": 66
			},
			{
				"cell_type": "code",
				"source": [
					"import numpy as np\n",
					"\n",
					"sequence_length = 30  # use past 30 days to predict next day\n",
					"X = []\n",
					"y = []\n",
					"\n",
					"for i in range(sequence_length, len(scaled_values)):\n",
					"    X.append(scaled_values[i-sequence_length:i])   # input sequence\n",
					"    y.append(scaled_values[i][3])  # target = 'Close' price (index 3)\n",
					"\n",
					"X = np.array(X)\n",
					"y = np.array(y)"
				],
				"execution_count": 67
			},
			{
				"cell_type": "code",
				"source": [
					"print(\"X shape:\", X.shape)  # Expected: (samples, 30, 8)\n",
					"print(\"y shape:\", y.shape)  # Expected: (samples,)"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"source": [
					"train_size = int(len(X) * 0.8)\n",
					"X_train, X_val = X[:train_size], X[train_size:]\n",
					"y_train, y_val = y[:train_size], y[train_size:]"
				],
				"execution_count": 73
			},
			{
				"cell_type": "code",
				"source": [
					"from tensorflow.keras.models import Sequential\n",
					"from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
					"from tensorflow.keras.callbacks import EarlyStopping\n",
					"from tensorflow.keras.optimizers import Adam\n",
					"\n",
					"model = Sequential()\n",
					"model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
					"model.add(Dropout(0.2))\n",
					"model.add(LSTM(32))\n",
					"model.add(Dropout(0.2))\n",
					"model.add(Dense(1))\n",
					"\n",
					"model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
					"\n",
					"early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
					"\n",
					"history = model.fit(\n",
					"    X_train, y_train,\n",
					"    validation_data=(X_val, y_val),\n",
					"    epochs=50,\n",
					"    batch_size=256,\n",
					"    callbacks=[early_stop],\n",
					"    verbose=1\n",
					")"
				],
				"execution_count": 74
			},
			{
				"cell_type": "code",
				"source": [
					"# Make predictions\n",
					"predictions = model.predict(X_test)"
				],
				"execution_count": 75
			},
			{
				"cell_type": "code",
				"source": [
					"# Only use the Close column for inverse transform\n",
					"# Create an array of zeros with the same shape as the original feature set\n",
					"predicted_prices = np.zeros((len(predictions), len(features)))\n",
					"actual_prices = np.zeros((len(y_test), len(features)))\n",
					"\n",
					"# Place the predicted and actual close prices in the correct column (index 3 for 'Close')\n",
					"predicted_prices[:, 3] = predictions[:, 0]\n",
					"actual_prices[:, 3] = y_test\n",
					"\n",
					"# Inverse transform\n",
					"predicted_close = scaler.inverse_transform(predicted_prices)[:, 3]\n",
					"actual_close = scaler.inverse_transform(actual_prices)[:, 3]"
				],
				"execution_count": 76
			},
			{
				"cell_type": "code",
				"source": [
					"import matplotlib.pyplot as plt\n",
					"\n",
					"plt.figure(figsize=(12,6))\n",
					"plt.plot(actual_close, label='Actual Closing Price')\n",
					"plt.plot(predicted_close, label='Predicted Closing Price')\n",
					"plt.title('LSTM Model â€” Apple Stock Price Prediction')\n",
					"plt.xlabel('Time')\n",
					"plt.ylabel('Price')\n",
					"plt.legend()\n",
					"plt.grid(True)\n",
					"plt.show()"
				],
				"execution_count": 77
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.metrics import mean_squared_error\n",
					"import numpy as np\n",
					"\n",
					"rmse = np.sqrt(mean_squared_error(actual_close, predicted_close))\n",
					"print(\"Root Mean Squared Error:\", rmse)"
				],
				"execution_count": 78
			},
			{
				"cell_type": "code",
				"source": [
					"import matplotlib.pyplot as plt\n",
					"\n",
					"plt.figure(figsize=(10, 5))\n",
					"plt.plot(history.history['loss'], label='Training Loss')\n",
					"plt.plot(history.history['val_loss'], label='Validation Loss')\n",
					"plt.title('Training vs Validation Loss')\n",
					"plt.xlabel('Epoch')\n",
					"plt.ylabel('MSE Loss')\n",
					"plt.legend()\n",
					"plt.grid(True)\n",
					"plt.show()\n",
					""
				],
				"execution_count": 79
			},
			{
				"cell_type": "code",
				"source": [
					"# Step 1: Make sure the date column is sorted and in datetime format\n",
					"df['date'] = pd.to_datetime(df['date'])\n",
					"df = df.sort_values('date')\n",
					"\n",
					"# Step 2: Get the dates corresponding to X (after window slicing)\n",
					"# Example: if X shape is (149086, 30, 8), that means we lose the first 30 dates\n",
					"valid_dates = df['date'].values[30:]\n",
					"\n",
					"# Step 3: Now split into train and test the same way you split X\n",
					"train_size = int(len(valid_dates) * 0.8)\n",
					"test_dates = valid_dates[train_size:]  # Aligns with X_test and y_test"
				],
				"execution_count": 83
			},
			{
				"cell_type": "code",
				"source": [
					"import matplotlib.pyplot as plt\n",
					"\n",
					"plt.figure(figsize=(12, 6))\n",
					"plt.plot(test_dates, y_test, label='Actual Closing Price')\n",
					"plt.plot(test_dates, predictions, label='Predicted Closing Price')\n",
					"plt.xlabel(\"Date\")\n",
					"plt.ylabel(\"Price\")\n",
					"plt.title(\"LSTM Model â€” Apple Stock Price Prediction\")\n",
					"plt.legend()\n",
					"plt.xticks(rotation=45)\n",
					"plt.tight_layout()\n",
					"plt.show()"
				],
				"execution_count": 85
			},
			{
				"cell_type": "code",
				"source": [
					"plt.figure(figsize=(12, 6))\n",
					"plt.plot(test_dates, actual_close, label='Actual Closing Price', color='blue')\n",
					"plt.plot(test_dates, predicted_close, label='Predicted Closing Price', color='orange')\n",
					"plt.title(\"LSTM Model â€” Apple Stock Price Prediction\")\n",
					"plt.xlabel(\"Date\")\n",
					"plt.ylabel(\"Price\")\n",
					"plt.legend()\n",
					"plt.xticks(rotation=45)\n",
					"plt.grid(True)\n",
					"plt.tight_layout()\n",
					"plt.show()"
				],
				"execution_count": 88
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.preprocessing import MinMaxScaler\n",
					"\n",
					"# Define only the numeric columns used during model training\n",
					"feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume', 'sentiment_news', 'sentiment_reviews', 'sentiment_search']\n",
					"\n",
					"# Select only those from the DataFrame\n",
					"features = df[feature_columns].astype(float).values  # <-- Ensures no timestamps\n",
					"\n",
					"# Fit or reuse your scaler\n",
					"scaler = MinMaxScaler()\n",
					"scaled_features = scaler.fit_transform(features)  # Or use scaler.transform(features) if already fitted\n",
					"\n",
					"# Define window size again if needed\n",
					"window_size = 30\n",
					"\n",
					"# Use last 30 days to predict the next\n",
					"last_window = scaled_features[-window_size:]\n",
					"input_data = np.expand_dims(last_window, axis=0)\n",
					"\n",
					"predicted_scaled_close = model.predict(input_data)\n",
					"\n",
					"temp_array = np.zeros((1, scaled_features.shape[1]))\n",
					"temp_array[0, ] = predicted_scaled_close  \n",
					"\n",
					"predicted_close_apr21 = scaler.inverse_transform(temp_array)[0, 3]\n",
					"print(f\"ðŸ“ˆ Predicted Closing Price for April 21, 2025: ${predicted_close_apr21:.2f}\")"
				],
				"execution_count": 95
			},
			{
				"cell_type": "code",
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}